{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "constants = imp.load_source('constants', '../src/constants.py')\n",
    "nTrain = constants.nTrain\n",
    "nVal = constants.nVal\n",
    "nTest = constants.nTest\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv',sep=';',index_col=0)\n",
    "val = pd.read_csv('../data/val.csv',sep=';',index_col=0)\n",
    "test = pd.read_csv('../data/test.csv',sep=';',index_col=0)\n",
    "ytrain = train['sentiment'].values\n",
    "yval = val['sentiment'].values\n",
    "ytest = test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(sentence, grams):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    for gram in grams:\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            tokens += [\"_*_\".join(words[i:i+gram])]\n",
    "    return tokens\n",
    "\n",
    "def build_dict(f, grams):\n",
    "    dic = Counter()\n",
    "    for sentence in open(f).xreadlines():\n",
    "        dic.update(tokenize(sentence, grams))\n",
    "    return dic\n",
    "\n",
    "def process_files(file_pos, file_neu, file_neg, dic, r, outfn, grams):\n",
    "    output = []\n",
    "    for beg_line, f in zip([\"1\", \"0\", \"-1\"], [file_pos, file_neu, file_neg]):\n",
    "        for l in open(f).xreadlines():\n",
    "            tokens = tokenize(l, grams)\n",
    "            indexes = []\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    indexes += [dic[t]]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            indexes = list(set(indexes))\n",
    "            indexes.sort()\n",
    "            line = [beg_line]\n",
    "            for i in indexes:\n",
    "                line += [\"%i:%f\" % (i + 1, r[i])]\n",
    "            output += [\" \".join(line)]\n",
    "    output = \"\\n\".join(output)\n",
    "    f = open(outfn, \"w\")\n",
    "    f.writelines(output)\n",
    "    f.close()\n",
    "\n",
    "def compute_ratio(poscounts, neucounts, negcounts, alpha=1):\n",
    "    alltokens = list(set(poscounts.keys() + neucounts.keys() + negcounts.keys()))\n",
    "    dic = dict((t, i) for i, t in enumerate(alltokens))\n",
    "    d = len(dic)\n",
    "    print \"computing r...\"\n",
    "    p, q = np.ones(d) * alpha , np.ones(d) * alpha\n",
    "    for t in alltokens:\n",
    "        p[dic[t]] += poscounts[t]\n",
    "        q[dic[t]] += negcounts[t]\n",
    "    p /= abs(p).sum()\n",
    "    q /= abs(q).sum()\n",
    "    r = np.log(p/q)\n",
    "    return dic, r\n",
    "\n",
    "def main(ptrain, neutrain, ntrain, ptest, neutest, ntest, out, liblinear, ngram):\n",
    "    ngram = [int(i) for i in ngram]\n",
    "    print \"counting...\"\n",
    "    poscounts = build_dict(ptrain, ngram) \n",
    "    neucounts = build_dict(neutrain, ngram)         \n",
    "    negcounts = build_dict(ntrain, ngram)         \n",
    "    \n",
    "    dic, r = compute_ratio(poscounts, neucounts, negcounts)\n",
    "    print \"processing files...\"\n",
    "    process_files(ptrain, ntrain, dic, r, \"train-nbsvm.txt\", ngram)\n",
    "    process_files(ptest, ntest, dic, r, \"test-nbsvm.txt\", ngram)\n",
    "    \n",
    "    trainsvm = os.path.join(liblinear, \"train\") \n",
    "    predictsvm = os.path.join(liblinear, \"predict\") \n",
    "    os.system(trainsvm + \" -s 0 train-nbsvm.txt model.logreg\")\n",
    "    os.system(predictsvm + \" -b 1 test-nbsvm.txt model.logreg \" + out)\n",
    "    os.system(\"rm model.logreg train-nbsvm.txt test-nbsvm.txt\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Usage :\n",
    "    python nbsvm.py --liblinear /PATH/liblinear-1.96\\\n",
    "        --ptrain /PATH/data/full-train-pos.txt\\\n",
    "        --ntrain /PATH/data/full-train-neg.txt\\\n",
    "        --ptest /PATH/data/test-pos.txt\\\n",
    "        --ntest /PATH/data/test-neg.txt\\\n",
    "         --ngram 123 --out TEST-SCORE\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Run NB-SVM on some text files.')\n",
    "    parser.add_argument('--liblinear', help='path of liblinear install e.g. */liblinear-1.96')\n",
    "    parser.add_argument('--ptrain', help='path of the text file TRAIN POSITIVE')\n",
    "    parser.add_argument('--ntrain', help='path of the text file TRAIN NEGATIVE')\n",
    "    parser.add_argument('--ptest', help='path of the text file TEST POSITIVE')\n",
    "    parser.add_argument('--ntest', help='path of the text file TEST NEGATIVE')\n",
    "    parser.add_argument('--out', help='path and fileename for score output')\n",
    "    parser.add_argument('--ngram', help='N-grams considered e.g. 123 is uni+bi+tri-grams')\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    main(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
